{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-IxNmlXULVa",
        "outputId": "08f99871-49da-4643-baaa-a921b00fba71"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTtIy34A9JYO",
        "outputId": "ed161009-a920-42ab-b7a3-8ecbf27afd22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 3.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=3eb02c5fe2f957b7d47ed04d33c989ffdddbe1f52f0832aa25c76974af975c0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c 11-785-s22-hw2p2-classification\n",
        "!kaggle competitions download -c 11-785-s22-hw2p2-verification\n",
        "\n",
        "!unzip -q 11-785-s22-hw2p2-classification.zip\n",
        "!unzip -q 11-785-s22-hw2p2-verification.zip\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQurdcTd9Te6",
        "outputId": "376971d8-93f8-4d64-b0ed-26c79f398c24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 11-785-s22-hw2p2-classification.zip to /content\n",
            "100% 2.34G/2.35G [01:09<00:00, 56.4MB/s]\n",
            "100% 2.35G/2.35G [01:09<00:00, 36.4MB/s]\n",
            "Downloading 11-785-s22-hw2p2-verification.zip to /content\n",
            " 94% 246M/263M [00:10<00:00, 24.7MB/s]\n",
            "100% 263M/263M [00:10<00:00, 26.6MB/s]\n",
            "11-785-s22-hw2p2-classification.zip   sample_data\n",
            "11-785-s22-hw2p2-verification.zip     train_subset\n",
            "classification\t\t\t      verification\n",
            "classification_sample_submission.csv  verification_sample_submission.csv\n",
            "drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CvKned_mKs7_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as ttf\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MKnENectKs8C"
      },
      "outputs": [],
      "source": [
        "class NextBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, expansion_ratio = 4):\n",
        "\n",
        "        super().__init__()\n",
        "        \n",
        "        hidden_dim = in_channels * expansion_ratio\n",
        "\n",
        "        self.depthWise = nn.Sequential (\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size = 7, padding = 3, bias = False, groups = in_channels),\n",
        "            nn.BatchNorm2d(in_channels)\n",
        "        )\n",
        "\n",
        "        self.pointwise = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size = 1, padding = 0, bias = False),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, in_channels, kernel_size = 1, padding = 0, bias = False),\n",
        "            nn.BatchNorm2d(in_channels)\n",
        "        )\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.depthWise(x)\n",
        "        out = self.pointwise(out)\n",
        "        out = self.bottleneck(out)\n",
        "        return x + out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size = 2, stride = 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.downsample(x)\n",
        "        return out "
      ],
      "metadata": {
        "id": "EifAnGPyYloS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9QquDmg9Ks8D"
      },
      "outputs": [],
      "source": [
        "class ConvNext(nn.Module):\n",
        "    \n",
        "    def __init__(self, classes = 7000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.classes = classes\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size = 4, padding = 0, bias = False, stride = 4),\n",
        "            nn.BatchNorm2d(96)\n",
        "        )\n",
        "\n",
        "        self.stages = [\n",
        "            [96, 4, 3],\n",
        "            [192, 4, 3],\n",
        "            [384, 4, 9],\n",
        "            [768, 4, 3],\n",
        "        ]\n",
        "\n",
        "        layers = self.make_layers()\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "        final_channels = 768\n",
        "\n",
        "        self.final_block = nn.Sequential(\n",
        "            nn.Conv2d(final_channels, final_channels, kernel_size=1, padding=0, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(final_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.cls_layer = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(final_channels, classes)\n",
        "        )\n",
        "\n",
        "    def make_layers(self):\n",
        "        layers = []\n",
        "\n",
        "        for idx, curr_stage in enumerate(self.stages):\n",
        "\n",
        "            in_channels, exp_ratio, num_blocks = curr_stage\n",
        "            for __ in range(num_blocks):\n",
        "                layers.append(NextBlock(\n",
        "                    in_channels = in_channels,\n",
        "                    expansion_ratio=exp_ratio))\n",
        " \n",
        "            if(idx != len(self.stages)-1):\n",
        "                out_channels = self.stages[idx+1][0]\n",
        "                layers.append(Downsample(in_channels = in_channels, out_channels = out_channels))\n",
        "            \n",
        "            \n",
        "        return layers\n",
        "\n",
        "\n",
        "    def forward(self, x, return_feats = False):\n",
        "        out = self.stem(x)\n",
        "        out = self.layers(out)\n",
        "        out = self.final_block(out)\n",
        "        if return_feats:\n",
        "            return out\n",
        "\n",
        "        out = self.cls_layer(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GbfRJpzKs8F"
      },
      "outputs": [],
      "source": [
        "# from torchsummary import summary\n",
        "# model = ConvNext(7000).cuda()\n",
        "# summary(model, (3, 224, 224))\n",
        "# del model\n",
        "# model = torch.load('/content/drive/MyDrive/HW2P2/models-conv/conv5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "chelcRpFKs8G"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "lr = 0.1\n",
        "epochs = 55\n",
        "\n",
        "DATA_DIR = \"/content\"\n",
        "TRAIN_DIR = osp.join(DATA_DIR, \"classification/classification/train\")\n",
        "VAL_DIR = osp.join(DATA_DIR, \"classification/classification/dev\")\n",
        "TEST_DIR = osp.join(DATA_DIR, \"classification/classification/test\")\n",
        "\n",
        "\n",
        "train_transforms = ttf.Compose([ttf.RandAugment(),\n",
        "                    ttf.RandomHorizontalFlip(),\n",
        "                    ttf.ColorJitter(),\n",
        "                    ttf.RandomRotation(15),\n",
        "                    ttf.RandomPerspective(0.3, 0.4),\n",
        "                    ttf.ToTensor(),\n",
        "                    ttf.RandomErasing(0.25),\n",
        "                    ttf.Normalize((0.51301944, 0.40335497, 0.35214797), (0.30744416, 0.2702129 , 0.25891313)), \n",
        "                    ])\n",
        "\n",
        "val_transforms = ttf.Compose([ttf.ToTensor(),\n",
        "                  ttf.Normalize((0.51301944, 0.40335497, 0.35214797), (0.30744416, 0.2702129 , 0.25891313))])\n",
        "\n",
        "val_dataset = torchvision.datasets.ImageFolder(VAL_DIR,\n",
        "                                               transform = val_transforms)\n",
        "train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR,\n",
        "                                                 transform = train_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True, num_workers=2)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        drop_last=True, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_trainable_parameters = 0\n",
        "for p in model.parameters():\n",
        "    num_trainable_parameters += p.numel()\n",
        "print(\"Number of Params: {}\".format(num_trainable_parameters))\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(label_smoothing = 0.1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4, nesterov = True)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones = [2,5,9,13,16,19,20,22,25,27,29] ,gamma = 0.5)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU52sErh-kQY",
        "outputId": "e3afd746-849e-45d5-c6f5-c80473013383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Params: 33811384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1,epochs+1):\n",
        "    # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "    model.train()\n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        total_loss += float(loss)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct,\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        \n",
        "        scaler.scale(loss).backward() \n",
        "        scaler.step(optimizer) \n",
        "        scaler.update() \n",
        "\n",
        "        batch_bar.update()\n",
        "    batch_bar.close() \n",
        "    \n",
        "    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
        "        epoch,\n",
        "        epochs,\n",
        "        100 * num_correct / (len(train_loader) * batch_size),\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "    num_correct = 0\n",
        "    for i, (x, y) in enumerate(val_loader):\n",
        "\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)))\n",
        "\n",
        "        batch_bar.update()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    print(\"Validation: {:.04f}%\".format(100 * num_correct / len(val_dataset)))\n",
        "    ss = '/content/drive/MyDrive/HW2P2/models-conv/conv'+str(epoch)\n",
        "    torch.save(model, ss)\n",
        "    \n",
        "    scheduler.step(100 * num_correct / len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "YdAPXp0IG7T_",
        "outputId": "e352b1cb-e67b-44cf-d874-07f6afadad79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30: Train Acc 96.4283%, Train Loss 1.6322, Learning Rate 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: 85.3114%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/30: Train Acc 96.6284%, Train Loss 1.6273, Learning Rate 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: 85.2743%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/30: Train Acc 96.5026%, Train Loss 1.6306, Learning Rate 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: 85.3457%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/30: Train Acc 96.5312%, Train Loss 1.6295, Learning Rate 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: 85.3314%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   5%|▍         | 54/1093 [00:57<18:01,  1.04s/it, acc=96.4915%, loss=1.6268, lr=0.0000, num_correct=6793]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8f3bb0c8246c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VerificationDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # We return the image, as well as the path to that image (relative path)\n",
        "        return self.transforms(Image.open(self.img_paths[idx])), osp.relpath(self.img_paths[idx], self.data_dir)"
      ],
      "metadata": {
        "id": "WEF39Sbb6f8g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/dev\"), val_transforms)\n",
        "val_ver_loader = torch.utils.data.DataLoader(val_veri_dataset, batch_size=batch_size, \n",
        "                                             shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "_KIk1Hzz6hbE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('/content/drive/MyDrive/HW2P2/models-conv/conv5')"
      ],
      "metadata": {
        "id": "QA4i1rQJ6IJU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "activation = nn.GELU()\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(val_ver_loader), total=len(val_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try the final outputs too!\n",
        "        feats = model(imgs, return_feats = False)\n",
        "        out = activation(feats)\n",
        "        for img_name, embedding in zip(path_names, out):\n",
        "            feats_dict['dev/'+img_name] = embedding\n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_olagGX6i5n",
        "outputId": "b6025ea2-8a0c-4c42-877e-e7fa523a6438"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "similarity_metric = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_dev.csv\")\n",
        "\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "gt_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2, gt = line.split(\",\")\n",
        "\n",
        "    # TODO: Use the similarity metric\n",
        "    # How to use these img_paths? What to do with the features?\n",
        "    similarity = similarity_metric(feats_dict[img_path1],feats_dict[img_path2])\n",
        "    # print(similarity)\n",
        "    pred_similarities.append(similarity.cpu().numpy())\n",
        "    gt_similarities.append(int(gt))\n",
        "\n",
        "pred_similarities = np.array(pred_similarities)\n",
        "gt_similarities = np.array(gt_similarities)\n",
        "# pred_similarities_avg = np.mean(pred_similarities, axis = (1,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQIGeZQA6kMD",
        "outputId": "f40c55d0-5983-433a-e844-95c95feeabd5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AUC:\", roc_auc_score(gt_similarities, pred_similarities))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajm7aMea6GtF",
        "outputId": "eb5ed786-cdb7-4f7d-9554-4b0e8380a4c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9685092547024112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/test\"), val_transforms)\n",
        "test_ver_loader = torch.utils.data.DataLoader(test_veri_dataset, batch_size=batch_size, \n",
        "                                              shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "8rFOZfLk6onr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "feats_dict = dict()\n",
        "activation = nn.GELU()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(test_ver_loader), total=len(test_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try to final outputs too!\n",
        "        feats = model(imgs, return_feats = False) \n",
        "        out = activation(feats)\n",
        "        for img_name, embedding in zip(path_names, out):\n",
        "            feats_dict['test/'+img_name] = embedding\n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCPHwvta6pE0",
        "outputId": "f484447e-7608-4389-820d-9af3ccb72651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "# similarity_metric = \n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_test.csv\")\n",
        "\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2 = line.split(\",\")\n",
        "\n",
        "    similarity = similarity_metric(feats_dict[img_path1],feats_dict[img_path2])\n",
        "    pred_similarities.append(similarity.cpu().numpy())\n",
        "    \n",
        "pred_similarities = np.array(pred_similarities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEOlcrNM6qeU",
        "outputId": "5e650c16-0d98-404b-e5c7-42109a95c267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"verification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,match\\n\")\n",
        "    for i in range(len(pred_similarities)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_similarities[i]))"
      ],
      "metadata": {
        "id": "jGpOGBMQ6stQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions submit -c 11-785-s22-hw2p2-verification -f verification_early_submission.csv -m \"NA\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq6xCqah6u83",
        "outputId": "c3ed45be-cf67-4c72-a1b8-7f35f62a7275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 16.9M/16.9M [00:00<00:00, 45.9MB/s]\n",
            "Successfully submitted to Face Verification"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H6xf8pwC_nqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "3325ce98b0e3092794368775f7268d48d8b7f22d032187877226117cae8da226"
    },
    "kernelspec": {
      "display_name": "Python 3.9.9 ('mambaforge')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Face Verification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}